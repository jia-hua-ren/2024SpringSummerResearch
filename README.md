# 2024SpringSummerResearch
Research/coding done in Spring/Summer 2024 at Osaka University, in Higo lab, under Prof. Yoshiki Higo.

To create functionally equivalent(FE) method pair dataset.

## Core Idea

The goal of this research is to support the development of improved code-clone detection systems by building a dataset of **Functionally Equivalent** C code function pairs. Two pieces of code are considered *functionally equivalent* if they:

- Have the same function signature (parameters and return type), and  
- Produce the same output for all valid inputs, regardless of differences in implementation or structure.

To verify functional equivalence, we generate unit tests for each function and perform **cross-testing**: we run the unit tests of one function on the other, and vice versa. If both functions pass each other’s tests, we classify them as functionally equivalent. This pair is then verified by human eyes and the dataset can then be used to train or evaluate code-clone detection systems.

---

## Challenges

### 1. Unit Testing in C/C++

The most significant challenge was finding a suitable unit testing tool for C/C++. After extensive searching and experimentation, I settled on [UnitTestBot](https://github.com/UnitTestBot/UTBotCpp). This process involved a steep learning curve, including setting up Docker environments, understanding the tool's limitations, and working through a variety of compatibility issues. The tool is powerful but not without its flaws.

For example, UnitTestBot has difficulty handling many outlined types of code on their docs, one that is particularily interesting are **ternary operators**. It is unable to generate test cases that exercise both branches of a ternary condition, which would limit the number of code samples we can verify, as I am sure many codes out there uses this.

### 2. Collecting Usable Code Samples

Another major hurdle was sourcing appropriate C functions. I initially created a script to clone top-ranking C repositories from GitHub and extract individual functions. However, many functions relied on **external variables**, **custom types**, or **non-standard libraries**, making them difficult or impossible to compile and test in isolation.

To address this, I filtered for **self-contained functions** —those that could compile and run with only standard library dependencies. I used Clang to run these functions and I had to write additional logic to:

- Automatically add standard `#include` statements where needed
- Exclude functions with external dependencies
- Normalize return types

Even then, issues persisted. Many functions had **complex or non-standard return types**, such as custom structs or pointers to void (`void*`). Early in the project, I didn’t fully understand the implications of `void*` usage, but after taking an Operating Systems course, I now recognize that `void*` often implies flexible data types cast dynamically—making it extremely difficult to verify true functional equivalence. As a result, I now believe that if I were to continue this project, these functions involving `void*` should be excluded entirely from the dataset.

## The contents of this Repo

c.py: This is the script that uses Clang to extract C/C++ methods from a C/C++ file. 
TODO: somehow detect macros in the preprocessing part and add those to the function extractions.
TODO: remove *any* use of user-defined types and only standard types from standard headers.

extract.py: this is the script that runs c.py on all files in a directory

cross_test.sh: this is for cross-testing. The script detects whether all tests are passed when ran on a specific method inside the group. 
TODO: need a way to store cross-testing results. Need to actually determine if the pair is FE (right now, it outputs "X and Y are functionally equivalent" when X passes Y's tests, but Y also needs to pass X's tests to be FE.

gen_tests.sh: a simple script that runs the UTBot cli generate test command on all files in a directory.

transform_test.py: This is the script that modifies a driver file for GoogleTest. The driver file is the one that allows test execution. How does it modify the driver file? This script extracts tests from the tests generated by UTBot (so run gen_tests first to get the tests), modifies the include to include the current method being ran, removes tests that results in error, modifies the tests to have the correct function name from the method file.

TODO: modify extract.py to rename functions to a generic name like \_\_test\_\_ to save work in transform_test.py. It removes the need to modify the tests to have correct function name for each test and method being ran.


General TODOs:
- in extract,py, save method informations to a sqlite3 db.
- etc... i cant think right now 
